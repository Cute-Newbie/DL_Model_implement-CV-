{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1EBd8O3HLqzGQ8tLDm4PJZNBDlAHQyOXL","authorship_tag":"ABX9TyM1DcK+X/Kt4nVwk3Fov9Mu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!python3 \"/content/drive/MyDrive/Mnist_classifier/Unet/data_read.py\""],"metadata":{"id":"NnufLt6wCakq","executionInfo":{"status":"ok","timestamp":1681380001487,"user_tz":-540,"elapsed":1827,"user":{"displayName":"김동윤","userId":"09997924893841539802"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","execution_count":46,"metadata":{"id":"531l-fA6y8QL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681380535639,"user_tz":-540,"elapsed":139872,"user":{"displayName":"김동윤","userId":"09997924893841539802"}},"outputId":"97f5bab8-0276-48c8-d526-024d95246af1"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'float'>\n","Learning rate ; 1.0000e-02\n","batch size: 2\n","num_epoch: 20\n","data_dir : /content/drive/MyDrive/Mnist_classifier/Unet/datasets\n","ckpt_dir : /content/drive/MyDrive/Mnist_classifier/Unet/checkpoint_v2\n","log_dir : /content/drive/MyDrive/Mnist_classifier/Unet/log_v2\n","result_dir : /content/drive/MyDrive/Mnist_classifier/Unet/result_v2\n","mode: train\n","/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0001 / 0012 | LOSS 0.7357\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0002 / 0012 | LOSS 0.8131\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0003 / 0012 | LOSS 0.5785\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0004 / 0012 | LOSS 0.4675\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0005 / 0012 | LOSS 0.3854\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0006 / 0012 | LOSS 0.3130\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0007 / 0012 | LOSS 0.2936\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0008 / 0012 | LOSS 0.2333\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0009 / 0012 | LOSS 0.2014\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0010 / 0012 | LOSS 0.1630\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0011 / 0012 | LOSS 0.1432\n","TRAIN: EPOCH 0001 / 0020 | BATCH 0012 / 0012 | LOSS 0.0979\n","VALID: EPOCH 0001 / 0020 | BATCH 0001 / 0002 | LOSS 327.7009\n","VALID: EPOCH 0001 / 0020 | BATCH 0002 / 0002 | LOSS 313.3791\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0001 / 0012 | LOSS 0.4453\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0002 / 0012 | LOSS 0.1320\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0003 / 0012 | LOSS -0.0210\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0004 / 0012 | LOSS -0.1901\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0005 / 0012 | LOSS -0.2427\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0006 / 0012 | LOSS -0.1513\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0007 / 0012 | LOSS -0.2080\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0008 / 0012 | LOSS -0.1446\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0009 / 0012 | LOSS -0.1992\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0010 / 0012 | LOSS -0.2485\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0011 / 0012 | LOSS -0.2006\n","TRAIN: EPOCH 0002 / 0020 | BATCH 0012 / 0012 | LOSS -0.2838\n","VALID: EPOCH 0002 / 0020 | BATCH 0001 / 0002 | LOSS 19.2135\n","VALID: EPOCH 0002 / 0020 | BATCH 0002 / 0002 | LOSS 6.3909\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0001 / 0012 | LOSS -1.2171\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0002 / 0012 | LOSS -1.2314\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0003 / 0012 | LOSS -1.2364\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0004 / 0012 | LOSS -1.2902\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0005 / 0012 | LOSS -1.2957\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0006 / 0012 | LOSS -1.3202\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0007 / 0012 | LOSS -1.4224\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0008 / 0012 | LOSS -1.4389\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0009 / 0012 | LOSS -1.4690\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0010 / 0012 | LOSS -1.5164\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0011 / 0012 | LOSS -1.5773\n","TRAIN: EPOCH 0003 / 0020 | BATCH 0012 / 0012 | LOSS -1.5568\n","VALID: EPOCH 0003 / 0020 | BATCH 0001 / 0002 | LOSS -0.0962\n","VALID: EPOCH 0003 / 0020 | BATCH 0002 / 0002 | LOSS -0.0742\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0001 / 0012 | LOSS -1.9262\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0002 / 0012 | LOSS -2.4203\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0003 / 0012 | LOSS -2.5261\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0004 / 0012 | LOSS -2.3091\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0005 / 0012 | LOSS -1.7081\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0006 / 0012 | LOSS -2.0247\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0007 / 0012 | LOSS -1.6609\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0008 / 0012 | LOSS -1.3951\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0009 / 0012 | LOSS -1.6431\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0010 / 0012 | LOSS -1.8956\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0011 / 0012 | LOSS -1.9666\n","TRAIN: EPOCH 0004 / 0020 | BATCH 0012 / 0012 | LOSS -1.7642\n","VALID: EPOCH 0004 / 0020 | BATCH 0001 / 0002 | LOSS 9.0077\n","VALID: EPOCH 0004 / 0020 | BATCH 0002 / 0002 | LOSS 10.0083\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0001 / 0012 | LOSS -4.1618\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0002 / 0012 | LOSS -3.6980\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0003 / 0012 | LOSS -3.8896\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0004 / 0012 | LOSS -3.9244\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0005 / 0012 | LOSS -4.1845\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0006 / 0012 | LOSS -4.1739\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0007 / 0012 | LOSS -4.1655\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0008 / 0012 | LOSS -4.1420\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0009 / 0012 | LOSS -4.1031\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0010 / 0012 | LOSS -3.6599\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0011 / 0012 | LOSS -3.2998\n","TRAIN: EPOCH 0005 / 0020 | BATCH 0012 / 0012 | LOSS -3.5155\n","VALID: EPOCH 0005 / 0020 | BATCH 0001 / 0002 | LOSS -0.6436\n","VALID: EPOCH 0005 / 0020 | BATCH 0002 / 0002 | LOSS -0.5695\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0001 / 0012 | LOSS 0.4092\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0002 / 0012 | LOSS -2.0392\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0003 / 0012 | LOSS -1.2777\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0004 / 0012 | LOSS -2.6038\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0005 / 0012 | LOSS -3.3355\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0006 / 0012 | LOSS -3.8927\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0007 / 0012 | LOSS -3.2898\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0008 / 0012 | LOSS -2.8342\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0009 / 0012 | LOSS -3.3854\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0010 / 0012 | LOSS -3.8811\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0011 / 0012 | LOSS -3.4989\n","TRAIN: EPOCH 0006 / 0020 | BATCH 0012 / 0012 | LOSS -3.6932\n","VALID: EPOCH 0006 / 0020 | BATCH 0001 / 0002 | LOSS 20.9376\n","VALID: EPOCH 0006 / 0020 | BATCH 0002 / 0002 | LOSS 24.0410\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0001 / 0012 | LOSS 0.3467\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0002 / 0012 | LOSS -3.1955\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0003 / 0012 | LOSS -4.4324\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0004 / 0012 | LOSS -5.2865\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0005 / 0012 | LOSS -5.6018\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0006 / 0012 | LOSS -5.9157\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0007 / 0012 | LOSS -6.3224\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0008 / 0012 | LOSS -5.4960\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0009 / 0012 | LOSS -5.6521\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0010 / 0012 | LOSS -5.8594\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0011 / 0012 | LOSS -6.2381\n","TRAIN: EPOCH 0007 / 0020 | BATCH 0012 / 0012 | LOSS -5.6971\n","VALID: EPOCH 0007 / 0020 | BATCH 0001 / 0002 | LOSS 47.0782\n","VALID: EPOCH 0007 / 0020 | BATCH 0002 / 0002 | LOSS 22.9525\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0001 / 0012 | LOSS -12.1087\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0002 / 0012 | LOSS -10.5804\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0003 / 0012 | LOSS -10.7674\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0004 / 0012 | LOSS -8.0272\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0005 / 0012 | LOSS -8.3328\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0006 / 0012 | LOSS -9.0609\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0007 / 0012 | LOSS -9.9031\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0008 / 0012 | LOSS -9.8611\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0009 / 0012 | LOSS -10.3642\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0010 / 0012 | LOSS -10.3227\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0011 / 0012 | LOSS -10.2268\n","TRAIN: EPOCH 0008 / 0020 | BATCH 0012 / 0012 | LOSS -10.1716\n","VALID: EPOCH 0008 / 0020 | BATCH 0001 / 0002 | LOSS 27.8126\n","VALID: EPOCH 0008 / 0020 | BATCH 0002 / 0002 | LOSS 13.4870\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0001 / 0012 | LOSS -12.5816\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0002 / 0012 | LOSS -5.9270\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0003 / 0012 | LOSS -9.6406\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0004 / 0012 | LOSS -7.0839\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0005 / 0012 | LOSS -8.7093\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0006 / 0012 | LOSS -9.0844\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0007 / 0012 | LOSS -9.5663\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0008 / 0012 | LOSS -10.4501\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0009 / 0012 | LOSS -10.7132\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0010 / 0012 | LOSS -11.5238\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0011 / 0012 | LOSS -12.3124\n","TRAIN: EPOCH 0009 / 0020 | BATCH 0012 / 0012 | LOSS -12.8526\n","VALID: EPOCH 0009 / 0020 | BATCH 0001 / 0002 | LOSS 8.8244\n","VALID: EPOCH 0009 / 0020 | BATCH 0002 / 0002 | LOSS 19.1893\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0001 / 0012 | LOSS -12.8595\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0002 / 0012 | LOSS -18.3538\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0003 / 0012 | LOSS -17.1223\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0004 / 0012 | LOSS -12.5136\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0005 / 0012 | LOSS -13.4308\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0006 / 0012 | LOSS -10.9802\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0007 / 0012 | LOSS -11.8230\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0008 / 0012 | LOSS -13.2428\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0009 / 0012 | LOSS -11.7061\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0010 / 0012 | LOSS -10.4854\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0011 / 0012 | LOSS -9.5006\n","TRAIN: EPOCH 0010 / 0020 | BATCH 0012 / 0012 | LOSS -10.2374\n","VALID: EPOCH 0010 / 0020 | BATCH 0001 / 0002 | LOSS 41.9444\n","VALID: EPOCH 0010 / 0020 | BATCH 0002 / 0002 | LOSS 48.5326\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0001 / 0012 | LOSS -28.1465\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0002 / 0012 | LOSS -23.0496\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0003 / 0012 | LOSS -22.1127\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0004 / 0012 | LOSS -16.1868\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0005 / 0012 | LOSS -17.6868\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0006 / 0012 | LOSS -14.6391\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0007 / 0012 | LOSS -12.3912\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0008 / 0012 | LOSS -10.7270\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0009 / 0012 | LOSS -9.5071\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0010 / 0012 | LOSS -11.1031\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0011 / 0012 | LOSS -10.0775\n","TRAIN: EPOCH 0011 / 0020 | BATCH 0012 / 0012 | LOSS -11.0250\n","VALID: EPOCH 0011 / 0020 | BATCH 0001 / 0002 | LOSS 83.8963\n","VALID: EPOCH 0011 / 0020 | BATCH 0002 / 0002 | LOSS 78.6895\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0001 / 0012 | LOSS -19.2820\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0002 / 0012 | LOSS -19.2871\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0003 / 0012 | LOSS -19.9525\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0004 / 0012 | LOSS -20.6165\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0005 / 0012 | LOSS -16.4792\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0006 / 0012 | LOSS -18.0406\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0007 / 0012 | LOSS -18.7968\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0008 / 0012 | LOSS -20.4044\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0009 / 0012 | LOSS -20.0843\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0010 / 0012 | LOSS -20.3944\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0011 / 0012 | LOSS -20.4637\n","TRAIN: EPOCH 0012 / 0020 | BATCH 0012 / 0012 | LOSS -20.8184\n","VALID: EPOCH 0012 / 0020 | BATCH 0001 / 0002 | LOSS 15.2319\n","VALID: EPOCH 0012 / 0020 | BATCH 0002 / 0002 | LOSS 35.6529\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0001 / 0012 | LOSS -23.0681\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0002 / 0012 | LOSS -25.0972\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0003 / 0012 | LOSS -25.1677\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0004 / 0012 | LOSS -18.8605\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0005 / 0012 | LOSS -15.0742\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0006 / 0012 | LOSS -17.9213\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0007 / 0012 | LOSS -15.3521\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0008 / 0012 | LOSS -17.8050\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0009 / 0012 | LOSS -18.5175\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0010 / 0012 | LOSS -16.6593\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0011 / 0012 | LOSS -18.5709\n","TRAIN: EPOCH 0013 / 0020 | BATCH 0012 / 0012 | LOSS -16.9955\n","VALID: EPOCH 0013 / 0020 | BATCH 0001 / 0002 | LOSS -4.0824\n","VALID: EPOCH 0013 / 0020 | BATCH 0002 / 0002 | LOSS -3.4774\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0001 / 0012 | LOSS -31.1360\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0002 / 0012 | LOSS -33.2963\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0003 / 0012 | LOSS -33.9312\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0004 / 0012 | LOSS -35.4979\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0005 / 0012 | LOSS -28.3698\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0006 / 0012 | LOSS -29.5612\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0007 / 0012 | LOSS -31.7679\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0008 / 0012 | LOSS -33.9801\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0009 / 0012 | LOSS -33.2721\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0010 / 0012 | LOSS -33.3441\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0011 / 0012 | LOSS -30.2693\n","TRAIN: EPOCH 0014 / 0020 | BATCH 0012 / 0012 | LOSS -27.7291\n","VALID: EPOCH 0014 / 0020 | BATCH 0001 / 0002 | LOSS 159.2409\n","VALID: EPOCH 0014 / 0020 | BATCH 0002 / 0002 | LOSS 148.9452\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0001 / 0012 | LOSS 0.1515\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0002 / 0012 | LOSS -26.3596\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0003 / 0012 | LOSS -17.5300\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0004 / 0012 | LOSS -20.9359\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0005 / 0012 | LOSS -26.2733\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0006 / 0012 | LOSS -31.0983\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0007 / 0012 | LOSS -33.8022\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0008 / 0012 | LOSS -34.0249\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0009 / 0012 | LOSS -35.2070\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0010 / 0012 | LOSS -35.2684\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0011 / 0012 | LOSS -32.0087\n","TRAIN: EPOCH 0015 / 0020 | BATCH 0012 / 0012 | LOSS -32.5830\n","VALID: EPOCH 0015 / 0020 | BATCH 0001 / 0002 | LOSS 55.4130\n","VALID: EPOCH 0015 / 0020 | BATCH 0002 / 0002 | LOSS 24.5467\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0001 / 0012 | LOSS -33.2733\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0002 / 0012 | LOSS -36.0969\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0003 / 0012 | LOSS -43.8802\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0004 / 0012 | LOSS -32.8743\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0005 / 0012 | LOSS -26.2675\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0006 / 0012 | LOSS -30.6658\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0007 / 0012 | LOSS -32.2164\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0008 / 0012 | LOSS -34.2708\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0009 / 0012 | LOSS -30.4410\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0010 / 0012 | LOSS -27.3559\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0011 / 0012 | LOSS -30.4389\n","TRAIN: EPOCH 0016 / 0020 | BATCH 0012 / 0012 | LOSS -33.2109\n","VALID: EPOCH 0016 / 0020 | BATCH 0001 / 0002 | LOSS 57.5119\n","VALID: EPOCH 0016 / 0020 | BATCH 0002 / 0002 | LOSS 24.9752\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0001 / 0012 | LOSS 0.5282\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0002 / 0012 | LOSS -28.5185\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0003 / 0012 | LOSS -18.9239\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0004 / 0012 | LOSS -32.0567\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0005 / 0012 | LOSS -33.9622\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0006 / 0012 | LOSS -40.2223\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0007 / 0012 | LOSS -43.5310\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0008 / 0012 | LOSS -44.6400\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0009 / 0012 | LOSS -39.6445\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0010 / 0012 | LOSS -35.6131\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0011 / 0012 | LOSS -37.7601\n","TRAIN: EPOCH 0017 / 0020 | BATCH 0012 / 0012 | LOSS -38.1768\n","VALID: EPOCH 0017 / 0020 | BATCH 0001 / 0002 | LOSS 318.9591\n","VALID: EPOCH 0017 / 0020 | BATCH 0002 / 0002 | LOSS 155.6235\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0001 / 0012 | LOSS -47.2039\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0002 / 0012 | LOSS -55.9620\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0003 / 0012 | LOSS -57.9585\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0004 / 0012 | LOSS -56.4236\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0005 / 0012 | LOSS -59.7440\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0006 / 0012 | LOSS -61.9766\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0007 / 0012 | LOSS -60.5793\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0008 / 0012 | LOSS -62.0141\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0009 / 0012 | LOSS -55.0356\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0010 / 0012 | LOSS -49.3665\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0011 / 0012 | LOSS -44.8440\n","TRAIN: EPOCH 0018 / 0020 | BATCH 0012 / 0012 | LOSS -45.8974\n","VALID: EPOCH 0018 / 0020 | BATCH 0001 / 0002 | LOSS 188.2909\n","VALID: EPOCH 0018 / 0020 | BATCH 0002 / 0002 | LOSS 90.3133\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0001 / 0012 | LOSS 0.1878\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0002 / 0012 | LOSS -29.3756\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0003 / 0012 | LOSS -36.9269\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0004 / 0012 | LOSS -27.5385\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0005 / 0012 | LOSS -39.1694\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0006 / 0012 | LOSS -42.1763\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0007 / 0012 | LOSS -47.4478\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0008 / 0012 | LOSS -49.5794\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0009 / 0012 | LOSS -49.7696\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0010 / 0012 | LOSS -44.7728\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0011 / 0012 | LOSS -45.8331\n","TRAIN: EPOCH 0019 / 0020 | BATCH 0012 / 0012 | LOSS -49.5222\n","VALID: EPOCH 0019 / 0020 | BATCH 0001 / 0002 | LOSS -8.3963\n","VALID: EPOCH 0019 / 0020 | BATCH 0002 / 0002 | LOSS -7.1770\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0001 / 0012 | LOSS -92.1592\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0002 / 0012 | LOSS -84.8646\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0003 / 0012 | LOSS -77.6734\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0004 / 0012 | LOSS -75.6749\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0005 / 0012 | LOSS -77.7386\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0006 / 0012 | LOSS -77.6223\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0007 / 0012 | LOSS -78.1717\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0008 / 0012 | LOSS -82.7186\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0009 / 0012 | LOSS -84.4572\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0010 / 0012 | LOSS -85.7866\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0011 / 0012 | LOSS -77.8971\n","TRAIN: EPOCH 0020 / 0020 | BATCH 0012 / 0012 | LOSS -81.4068\n","VALID: EPOCH 0020 / 0020 | BATCH 0001 / 0002 | LOSS -5.6833\n","VALID: EPOCH 0020 / 0020 | BATCH 0002 / 0002 | LOSS -4.5221\n"]}],"source":["!python3 \"/content/drive/MyDrive/Mnist_classifier/Unet/train.py\" \\\n","--lr 1e-2 --batch_size 2 --num_epoch 20 \\\n","--data_dir \"/content/drive/MyDrive/Mnist_classifier/Unet/datasets\" \\\n","--ckpt_dir \"/content/drive/MyDrive/Mnist_classifier/Unet/checkpoint_v2\" \\\n","--log_dir \"/content/drive/MyDrive/Mnist_classifier/Unet/log_v2\" \\\n","--result_dir \"/content/drive/MyDrive/Mnist_classifier/Unet/result_v2\" \\\n","--mode \"train\" \\\n","--train_continue \"on\""]},{"cell_type":"code","source":["\n","!python3 \"/content/drive/MyDrive/Mnist_classifier/Unet/train.py\" \\\n","--lr 1e-2 --batch_size 2 --num_epoch 300 \\\n","--data_dir \"/content/drive/MyDrive/Mnist_classifier/Unet/datasets\" \\\n","--ckpt_dir \"/content/drive/MyDrive/Mnist_classifier/Unet/checkpoint_v2\" \\\n","--log_dir \"/content/drive/MyDrive/Mnist_classifier/Unet/log_v2\" \\\n","--result_dir \"/content/drive/MyDrive/Mnist_classifier/Unet/result_v2\" \\\n","--mode \"test\" \\\n","--train_continue \"off\""],"metadata":{"id":"VaEVPtDf0J6G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681380867333,"user_tz":-540,"elapsed":9853,"user":{"displayName":"김동윤","userId":"09997924893841539802"}},"outputId":"b19a4751-1e9e-4dd9-dfc1-047f9b91b2f0"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'float'>\n","Learning rate ; 1.0000e-02\n","batch size: 2\n","num_epoch: 300\n","data_dir : /content/drive/MyDrive/Mnist_classifier/Unet/datasets\n","ckpt_dir : /content/drive/MyDrive/Mnist_classifier/Unet/checkpoint_v2\n","log_dir : /content/drive/MyDrive/Mnist_classifier/Unet/log_v2\n","result_dir : /content/drive/MyDrive/Mnist_classifier/Unet/result_v2\n","mode: test\n","/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","TEST: BATCH 0001 / 0002 | LOSS 0.7279\n","TEST: BATCH 0002 / 0002 | LOSS 0.7270\n","AVERAGE TEST: BATCH 0002 / 0002 | LOSS 0.7270\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GiSa9I_M0J7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Mhgb1KyV0J97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B4wdFPCC0KAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AbbYJbh10KCp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_OYB4cnV0KFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8X-1iGsQ0KHP"},"execution_count":null,"outputs":[]}]}